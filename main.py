import os
import asyncio
from dotenv import load_dotenv

from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional

# Langchain imports for RAG functionality
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Load environment variables from .env file
load_dotenv()

# --- Configuration & Setup ---
API_BEARER_TOKEN = os.getenv("API_BEARER_TOKEN", "74b1158d301e42af454a706d7610b664511de7b16c859c882a6bbb02cc936ed8")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY environment variable is not set. Please add it to your .env file.")

PDF_PATH = "policy.pdf"

app = FastAPI(
    title="LLM-Powered Intelligent Query–Retrieval System (Google Gemini)",
    description="API for processing large documents and making contextual decisions in insurance, legal, HR, and compliance domains.",
    version="1.0.0",
    docs_url="/api/v1/docs",
    redoc_url="/api/v1/redoc"
)

qa_chain: Optional[RetrievalQA] = None
vector_store: Optional[FAISS] = None

security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    if credentials.credentials != API_BEARER_TOKEN:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return credentials.credentials

class QueryRequest(BaseModel):
    documents: Optional[str] = None
    questions: List[str]

class QueryResponse(BaseModel):
    answers: List[str]

@app.on_event("startup")
async def startup_event():
    global qa_chain, vector_store

    print("--- Application Startup: Initializing RAG System ---")

    if not os.path.exists(PDF_PATH):
        print(f"ERROR: '{PDF_PATH}' not found. Please ensure it's generated by 'merge_pdfs.py' and placed in the root directory.")
        return

    try:
        print(f"Loading document from: {PDF_PATH}")
        loader = PyPDFLoader(PDF_PATH)
        documents = loader.load()
        print(f"Loaded {len(documents)} pages from {PDF_PATH}")

        print("Splitting documents into chunks...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        docs = text_splitter.split_documents(documents)
        print(f"Created {len(docs)} text chunks.")

        print("Creating embeddings and building FAISS vector store (this may take a moment)...")
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GOOGLE_API_KEY)
        vector_store = FAISS.from_documents(docs, embeddings)
        print("FAISS vector store built successfully.")

        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0, google_api_key=GOOGLE_API_KEY)

        # Final, corrected Prompt Template for strict, accurate output
        prompt_template = """
        You are an expert policy analyst. Your goal is to answer a list of questions about a set of combined insurance policies.
        For each question, extract the most relevant information directly from the provided context and present it in a clear, single-paragraph format.
        Start the answer with a phrase like "Yes, the policy covers..." or a direct factual statement. Do not add any conversational phrases, greetings, or explanations.
        When asked for conditions, include specific details like waiting periods, co-payments, sub-limits, and exclusions.
        If the necessary information to answer a question is not available in the context, respond with "Information not found in the provided documents.".

        Context:
        {context}

        Question:
        {question}

        Answer:
        """
        CUSTOM_PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
            chain_type_kwargs={"prompt": CUSTOM_PROMPT}
        )
        print("RetrievalQA chain initialized. API is ready to receive requests.")

    except Exception as e:
        print(f"--- ERROR during RAG System Initialization: {e} ---")
        print("Please ensure your GOOGLE_API_KEY is correct and 'policy.pdf' exists.")
        return

@app.post(
    "/hackrx/run",
    response_model=QueryResponse,
    dependencies=[Depends(verify_token)],
    summary="Run LLM-Powered Query-Retrieval on Policy Documents"
)
async def run_submission(request_body: QueryRequest):
    if qa_chain is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="RAG system is not initialized. Please check server logs for startup errors."
        )
    print(f"\n--- Received API Request ---")
    print(f"Questions: {request_body.questions}")

    try:
        # Use a list of tasks to run LLM calls concurrently for low latency
        tasks = [qa_chain.ainvoke({"query": question}) for question in request_body.questions]
        results = await asyncio.gather(*tasks) # This runs all the tasks in parallel
        
        answers = []
        for result in results:
            answers.append(result.get("result", "Information not found in the provided documents."))

        print("--- All questions processed. Sending response. ---")
        return {"answers": answers}
    except Exception as e:
        print(f"ERROR: An unexpected error occurred during query processing: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An internal server error occurred: {e}"
        )

@app.get("/", include_in_schema=False)
def root():
    return {"message": "LLM-Powered Intelligent Query–Retrieval System API is running. Visit /api/v1/docs for interactive documentation."}
