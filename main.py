import os
from dotenv import load_dotenv

from fastapi import FastAPI, Request, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional

# Langchain imports for RAG functionality
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

# Load environment variables from .env file
load_dotenv()

# --- Configuration & Setup ---
# Bearer token for API authentication as provided in the problem statement
API_BEARER_TOKEN = os.getenv("API_BEARER_TOKEN", "74b1158d301e42af454a706d7610b664511de7b16c859c882a6bbb02cc936ed8")

# Google API Key is essential for Google Gemini models
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY environment variable is not set. Please add it to your .env file.")

# Define the path to the merged PDF document
# This PDF is expected to be in the root of the project directory
PDF_PATH = "policy.pdf"

# --- FastAPI App Initialization ---
app = FastAPI(
    title="LLM-Powered Intelligent Query–Retrieval System (Google Gemini)",
    description="API for processing large documents and making contextual decisions in insurance, legal, HR, and compliance domains.",
    version="1.0.0",
    docs_url="/api/v1/docs",
    redoc_url="/api/v1/redoc"
)

# --- Global RAG Chain Components ---
# These will be initialized once on application startup for efficiency
qa_chain: Optional[RetrievalQA] = None
vector_store: Optional[FAISS] = None

# --- API Authentication Dependency ---
security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """
    Verifies the bearer token for API authentication.
    """
    if credentials.credentials != API_BEARER_TOKEN:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return credentials.credentials

# --- Pydantic Models for API Request/Response ---
class QueryRequest(BaseModel):
    # The 'documents' field is kept for API compatibility with the hackathon platform,
    # but this implementation uses a local 'policy.pdf' and ignores this URL.
    documents: Optional[str] = None
    questions: List[str]

class QueryResponse(BaseModel):
    answers: List[str]

# --- Application Startup Event ---
@app.on_event("startup")
async def startup_event():
    """
    Initializes the RAG components (document loading, chunking, embedding, FAISS index, QA chain)
    once when the FastAPI application starts.
    """
    global qa_chain, vector_store

    print("--- Application Startup: Initializing RAG System ---")

    # 1. Verify PDF file existence
    if not os.path.exists(PDF_PATH):
        print(f"ERROR: '{PDF_PATH}' not found. Please ensure it's generated by 'merge_pdfs.py' and placed in the root directory.")
        # In a production system, you might want to raise an exception here
        # or have a more robust way to handle missing documents.
        return

    try:
        # 2. Load the document
        print(f"Loading document from: {PDF_PATH}")
        loader = PyPDFLoader(PDF_PATH)
        documents = loader.load()
        print(f"Loaded {len(documents)} pages from {PDF_PATH}")

        # 3. Split documents into chunks
        print("Splitting documents into chunks...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        docs = text_splitter.split_documents(documents)
        print(f"Created {len(docs)} text chunks.")

        # 4. Create embeddings and build FAISS vector store
        print("Creating embeddings and building FAISS vector store (this may take a moment)...")
        # Ensure GOOGLE_API_KEY is passed for embeddings
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GOOGLE_API_KEY)
        vector_store = FAISS.from_documents(docs, embeddings)
        print("FAISS vector store built successfully.")

        # 5. Initialize the ChatGoogleGenerativeAI LLM
        # Ensure GOOGLE_API_KEY is passed for the chat model
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0, google_api_key=GOOGLE_API_KEY)
        
        # 6. Create the RetrievalQA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff", # "stuff" combines all retrieved documents into one prompt
            retriever=vector_store.as_retriever()
        )
        print("RetrievalQA chain initialized. API is ready to receive requests.")

    except Exception as e:
        print(f"--- ERROR during RAG System Initialization: {e} ---")
        print("Please ensure your GOOGLE_API_KEY is correct and 'policy.pdf' exists.")
        # Optionally, re-raise the exception to prevent the server from starting if initialization fails
        # raise

# --- API Endpoint ---
@app.post(
    "/hackrx/run", # Endpoint path as specified in the problem statement
    response_model=QueryResponse,
    dependencies=[Depends(verify_token)], # Apply authentication to this endpoint
    summary="Run LLM-Powered Query-Retrieval on Policy Documents"
)
async def run_submission(request_body: QueryRequest):
    """
    Processes a list of natural language questions against the pre-loaded policy document
    and returns contextual answers.

    **Note:** This implementation uses a local `policy.pdf` file. The `documents` URL
    provided in the request body is ignored.
    """
    if qa_chain is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="RAG system is not initialized. Please check server logs for startup errors."
        )

    print(f"\n--- Received API Request ---")
    print(f"Questions: {request_body.questions}")

    try:
        answers = []
        for question in request_body.questions:
            print(f"Processing question: '{question}'")
            # Invoke the QA chain with the user's question
            result = qa_chain.invoke({"query": question})
            # The result from RetrievalQA.invoke is a dictionary, typically with a 'result' key
            answers.append(result.get("result", "Could not retrieve an answer."))
            print(f"Answer generated for '{question}'.")

        print("--- All questions processed. Sending response. ---")
        return {"answers": answers}

    except Exception as e:
        print(f"ERROR: An unexpected error occurred during query processing: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An internal server error occurred: {e}"
        )

# --- Root Endpoint (Optional, for quick health check) ---
@app.get("/", include_in_schema=False)
def root():
    """
    Root endpoint to check if the API is running.
    """
    return {"message": "LLM-Powered Intelligent Query–Retrieval System API is running. Visit /api/v1/docs for interactive documentation."}

